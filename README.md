# Tech Challenge Fase 3 - Fine-tuning com AmazonTitles-1.3MM

## ğŸ“– DescriÃ§Ã£o do Projeto

Este projeto foi desenvolvido como parte do **Tech Challenge da Fase 3** do curso de IA para Devs da FIAP. O objetivo Ã© realizar o fine-tuning de um foundation model utilizando o dataset AmazonTitles-1.3MM, criando um sistema capaz de gerar descriÃ§Ãµes detalhadas de produtos a partir de seus tÃ­tulos.

## ğŸ¯ Objetivos

- Executar fine-tuning do modelo **LLaMA-3 8B** quantizado em 4 bits
- Utilizar o dataset **AmazonTitles-1.3MM** (arquivo `trn.json`)
- Implementar tÃ©cnica **LoRA** (Low-Rank Adaptation) para treinamento eficiente
- Criar sistema de inferÃªncia para responder perguntas sobre produtos
- Comparar performance antes e depois do fine-tuning

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.10+**
- **PyTorch**
- **Transformers (Hugging Face)**
- **Unsloth** - OtimizaÃ§Ã£o para fine-tuning
- **LoRA/PEFT** - AdaptaÃ§Ã£o eficiente de parÃ¢metros
- **TRL** - Treinamento supervisionado
- **Google Colab** - Ambiente de desenvolvimento

## ğŸ“ Estrutura do Projeto

```
tech-challenge-fine-tuning/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ tech_challenge_fine_tunning.ipynb    # Notebook principal
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trn.json                             # Dataset original
â”‚   â””â”€â”€ data.json                            # Dataset formatado
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lora_data/                           # Modelo fine-tunado
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â”œâ”€â”€ adapter_model.safetensors
â”‚       â””â”€â”€ tokenizer files...
â”‚
â”œâ”€â”€ outputs/                                 # Logs de treinamento
â”‚
â”œâ”€â”€ README.md                                # Este arquivo
â””â”€â”€ requirements.txt                         # DependÃªncias
```

## ğŸš€ Como Executar

### 1. ConfiguraÃ§Ã£o do Ambiente

**No Google Colab:**

```python
# Montar Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Instalar dependÃªncias
!pip install "unsloth[colab-new]" -U
!pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes
!pip install transformers datasets
```

### 2. Download do Dataset

1. Baixe o dataset AmazonTitles-1.3MM do [Google Drive](https://drive.google.com/file/d/12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK/view)
2. Extraia o arquivo `trn.json`
3. Coloque no seu Google Drive na pasta: `/content/drive/MyDrive/fine-tunning-fiap/`

### 3. ExecuÃ§Ã£o do Notebook

1. Abra o arquivo `tech_challenge_fine_tunning.ipynb` no Google Colab
2. Execute todas as cÃ©lulas sequencialmente
3. Aguarde o processo de fine-tuning (aproximadamente 30-60 minutos)

## ğŸ“Š Processo de Fine-tuning

### PrÃ©-processamento dos Dados

```python
# Formato do dataset original
{
  "title": "Nome do produto",
  "content": "DescriÃ§Ã£o detalhada do produto"
}

# Formato apÃ³s processamento (estilo Alpaca)
{
  "instruction": "Describe book as accurately as possible.",
  "input": "Question: Could you give me a description of the book 'Nome do Produto'?",
  "output": "DescriÃ§Ã£o detalhada do produto"
}
```

### ConfiguraÃ§Ã£o do Modelo

- **Modelo Base:** `unsloth/llama-3-8b-bnb-4bit`
- **TÃ©cnica:** LoRA (Low-Rank Adaptation)
- **ParÃ¢metros LoRA:**
  - `r = 16`
  - `lora_alpha = 32`
  - `lora_dropout = 0.05`
- **SequÃªncia mÃ¡xima:** 2048 tokens

### ParÃ¢metros de Treinamento

```python
TrainingArguments(
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 4,
    warmup_steps = 5,
    max_steps = 100,
    learning_rate = 2e-4,
    fp16 = True,  # ou bf16 se suportado
    logging_steps = 1,
    optim = "adamw_8bit",
    weight_decay = 0.01,
    lr_scheduler_type = "linear"
)
```

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Exemplo de Uso

```python
# Template de prompt
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
Describe book as accurately as possible.

### Input:
Question: Could you give me a description of the book 'Clean Architecture'?

### Response:
"""

# InferÃªncia
outputs = pipe(
    alpaca_prompt,
    max_new_tokens=256,
    temperature=0.7,
    top_k=50,
    top_p=0.95
)
```

### ComparaÃ§Ã£o Antes vs Depois

O notebook inclui testes comparativos mostrando:
- **Modelo Base:** Respostas genÃ©ricas e pouco especÃ­ficas
- **Modelo Fine-tunado:** DescriÃ§Ãµes detalhadas e contextualizadas

## ğŸ“ˆ Resultados

### Melhorias Observadas

1. **Especificidade:** Respostas mais especÃ­ficas sobre produtos
2. **CoerÃªncia:** Maior coerÃªncia temÃ¡tica nas descriÃ§Ãµes
3. **Detalhamento:** DescriÃ§Ãµes mais ricas e informativas
4. **ContextualizaÃ§Ã£o:** Melhor compreensÃ£o do contexto do produto

### MÃ©tricas de Performance

- **Tempo de treinamento:** ~45 minutos (Google Colab T4)
- **Uso de memÃ³ria:** ~8GB VRAM
- **ParÃ¢metros treinÃ¡veis:** ~4.2M (LoRA)
- **Dataset utilizado:** 1M exemplos formatados

## ğŸ¥ DemonstraÃ§Ã£o

O projeto inclui uma demonstraÃ§Ã£o em vÃ­deo mostrando:

1. **Carregamento do dataset**
2. **Processo de fine-tuning**
3. **ComparaÃ§Ã£o antes/depois**
4. **Testes interativos**
5. **Resultados finais**

**ğŸ”— Link do vÃ­deo:** [Inserir link do YouTube]

## ğŸ“‹ Requisitos TÃ©cnicos

### Hardware Recomendado

- **GPU:** NVIDIA T4 ou superior (disponÃ­vel no Google Colab)
- **RAM:** 12GB+ (sistema)
- **VRAM:** 8GB+ (GPU)
- **Armazenamento:** 10GB+ livre

### Software

```txt
torch>=2.0.0
transformers>=4.36.0
datasets>=2.14.0
unsloth[colab-new]
peft>=0.7.0
trl<0.9.0
accelerate>=0.24.0
bitsandbytes>=0.41.0
xformers
```

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### OtimizaÃ§Ãµes de MemÃ³ria

```python
# Para GPUs com menos memÃ³ria
per_device_train_batch_size = 1
gradient_accumulation_steps = 8
max_seq_length = 1024
```

### Ajuste de HiperparÃ¢metros

```python
# Para convergÃªncia mais rÃ¡pida
learning_rate = 5e-4
max_steps = 50

# Para melhor qualidade
learning_rate = 1e-4
max_steps = 200
```

## ğŸ”§ Troubleshooting

### Problemas Comuns

1. **Erro de memÃ³ria CUDA:**
   - Reduza `per_device_train_batch_size`
   - Aumente `gradient_accumulation_steps`

2. **Dataset nÃ£o encontrado:**
   - Verifique o caminho do arquivo `trn.json`
   - Confirme se o Google Drive estÃ¡ montado

3. **Modelo nÃ£o carrega:**
   - Verifique conexÃ£o com internet
   - Reinicie o runtime do Colab

## ğŸ‘¥ Equipe

- **Bruno Lima da Cruz**
- **Matheus Braz Giudice dos Santos**
- **Mislene Dalila da Silva**

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins educacionais como parte do Tech Challenge da FIAP.

## ğŸ”— Links Ãšteis

- **Dataset:** [AmazonTitles-1.3MM](https://drive.google.com/file/d/12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK/view)
- **Unsloth:** [GitHub](https://github.com/unslothai/unsloth)
- **Transformers:** [DocumentaÃ§Ã£o](https://huggingface.co/docs/transformers)
- **LoRA:** [Paper Original](https://arxiv.org/abs/2106.09685)


---

**ğŸ† Tech Challenge Fase 3 - IA para Devs - FIAP**
